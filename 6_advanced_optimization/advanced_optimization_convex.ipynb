{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# IAP 2024: Advanced Optimization Session\n",
    "\n",
    "This notebook is built based on the material developed by [Shuvomoy Das Gupta](https://github.com/Shuvomoy/MIT_IAP_2022_15_S60_Advanced_Optimization_Session_Shuvo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg;\n",
    "# Pkg.add.([\"Convex\", \"JuMP\", \"Images\", \"DelimitedFiles\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover the following topic today.\n",
    "\n",
    "# Topics\n",
    "\n",
    "---\n",
    "$\\textsf{Session 1: Convex Optimization and Extension}$\n",
    "---\n",
    "\n",
    "* How to model an optimization problem in a tractable manner\n",
    "\n",
    "\n",
    "* Tractable optimization models\n",
    "\n",
    "\n",
    "* Convex optimization through `Convex.jl` and `JuMP`\n",
    "\n",
    "\n",
    "* Modeling examples of common convex programs that shows up in OR\n",
    "\n",
    "\n",
    "* What to do when the problem is nonconvex\n",
    "\n",
    "\n",
    "---\n",
    "$\\textsf{Session 2: Discrete Optimization: Basic and Advanced}$\n",
    "---\n",
    "\n",
    "* Mixed Integer Linear Programs (MILP): basic\n",
    "\n",
    "\n",
    "* How MILPs are solved: Branch-and-Bound algorithm\n",
    "\n",
    "\n",
    "* How do we exploit structures to speed up MILPs\n",
    "\n",
    "\n",
    "* Solving Mixed Integer Quadratic Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling an optimization problem \n",
    "\n",
    "* We have our research problem, that usually come from a practical problem rooted in reality. \n",
    "\n",
    "* While modeling the problem, we want to stay as close to reality as possible, but without making the model intractable. \n",
    "\n",
    "* This is a balance. Safety-critical constraints have to be modeled exactly, where soft constraints can be relaxed.\n",
    "\n",
    "* It is a good practice to be considerate during the modeling phase so we end up with a model that is practically tractable.\n",
    "\n",
    "### Some good resources on how to model optimization problems\n",
    "\n",
    "* Model building in mathematical programming by H Williams ([Link](https://www.amazon.com/Model-Building-Mathematical-Programming-Williams/dp/1118443330))\n",
    "  * A very practical book for modeling LP and MILP\n",
    "  \n",
    "\n",
    "* Optimization models by G Calafiore and L El Ghaoui ([Link](https://www.amazon.com/Optimization-Models-Giuseppe-C-Calafiore/dp/1107050871))\n",
    "  * Good book for modeling convex optimization problems, especially Chapter 9, 10, 11  \n",
    "  \n",
    "  \n",
    "  \n",
    "* Applications of optimization with Xpress-MP by C Guéret, C Prins, and M Sevaux ([Link](https://www2.isye.gatech.edu/~sahmed/isye3133b/XpressBook.pdf))\n",
    "  * Excellent for modeling integer optimization models, especially Chapter 3, with lot of shortcuts and hacks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling an optimization problems\n",
    "\n",
    "At present the following types of problems are *practically tractable*. Whenever we are modeling an optmization problem, we should try to ensure that our model is one of the following:\n",
    "\n",
    "1. Linear programs (LP)\n",
    "\n",
    "\n",
    "\n",
    "2. Quadratic convex programs (QCP)\n",
    "\n",
    "\n",
    "\n",
    "3. Second order cone programs (SOCP)\n",
    "\n",
    "\n",
    "\n",
    "4. Semidefinite programs (SDP)\n",
    "\n",
    "\n",
    "\n",
    "5. Mixed-integer linear programs (MILP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My modeling strategies\n",
    "\n",
    "* **Solve the smaller optimization problem first**\n",
    "\n",
    "> Thomson's Rule for First-Time Telescope Makers: \"It is faster to *make a four-inch mirror then a six-inch mirror* than to *make a six-inch mirror*.\"\n",
    "\n",
    "I follow a modified version of Thomson's rule\n",
    "\n",
    "It is faster to **solve a smaller simpler optimization model and then the larger complicated optimization model** than to *solve a larger complicated optimization model*.\n",
    "\n",
    "* **Do not optimize prematurely**\n",
    "\n",
    "> \"The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.” \n",
    "> - Donald Knuth, The Art of Computer Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestones in optimization\n",
    "\n",
    "\n",
    "* 1947: G. Dantzig, who works for US air-forces, presents the Simplex method for solving LP-problems\n",
    "\n",
    "\n",
    "\n",
    "* 1948: J. Von Neumann establishes the theory of duality for LP-problems\n",
    "\n",
    "\n",
    "\n",
    "* 1951: H.W. Kuhn and A.W. Tucker reinvent Karush's optimality conditions (known as KKT conditions) \n",
    "\n",
    "\n",
    "\n",
    "* 1951: H. Markowitz presents his portfolio optimization theory => (1990 Nobel prize)\n",
    "\n",
    "\n",
    "\n",
    "* 1954: L.R. Ford's and D.R. Fulkerson's research on network problems => start of combinatorial optimization\n",
    "\n",
    "\n",
    "\n",
    "* 1960-1970: Many of the early works on first-order optimization algorithms are done (mostly developed in Soviet Union)\n",
    "\n",
    "\n",
    "\n",
    "* 1983: Nesterov comes up with accelerated gradient descent\n",
    "\n",
    "\n",
    "\n",
    "* 1984: N. Karmarkar's polynomial time algorithm for LP-problems begins a boom period for interior point methods\n",
    "\n",
    "\n",
    "\n",
    "* 1990s: Semidefinite optimization theory\n",
    "\n",
    "\n",
    "* 2010-present: First-order methods become very hot again due to machine learning\n",
    "\n",
    "\n",
    "* 2014: Performance estimation problem: computer-assisted design and analysis of optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## General convex optimization problem\n",
    "\n",
    "Most authoritative reference is Convex Optimization by Boyd and Vandenberghe ([pdf](https://web.stanford.edu/~boyd/cvxbook/)).\n",
    "\n",
    "Convex optimization problems are defined through the notion of convex set.\n",
    "\n",
    "**Convex set**\n",
    "\n",
    "<img src=\"figures/convex/convex_set.png\" alt=\"Drawing1\" style=\"width: 700px;\"/>\n",
    "\n",
    "**Convex function**\n",
    "\n",
    "A function  is convex if and only if the region above its graph is a convex set.\n",
    "\n",
    "\n",
    "<img src=\"figures/convex/convex_function.png\" alt=\"Drawing1\" style=\"width: 500px;\"/>\n",
    "\n",
    "## Standard form of a convex optimization problem\n",
    "\n",
    "A general convex optimization problem has\n",
    "the following form:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\underset{x\\in\\mathbf{R}^{d}}{\\mbox{minimize}} & f_{0}(x)\\\\\n",
    "\\mbox{subject to} & a_{i}^{\\top}x=b_{i},\\quad i=1,\\ldots,p,\\\\\n",
    " & f_{i}(x)\\leq0,\\quad i=1,\\ldots,m.\n",
    "\\end{array}\n",
    "$$\n",
    " where the equality constraints are linear and the functions $f_{0},f_{1},\\ldots,f_{m}$\n",
    "are convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex optimization\n",
    "\n",
    "* a subclass of optimization problems that includes LP as special case\n",
    "\n",
    "\n",
    "* convex problems can look very difficult (nonlinear, even\n",
    "nondifferentiable), but like LP can be solved very efficiently\n",
    "\n",
    "\n",
    "* convex problems come up more often than was once thought\n",
    "\n",
    "\n",
    "* many applications recently discovered in control, combinatorial\n",
    "optimization, signal processing, communications, circuit design,\n",
    "machine learning, statistics, finance, . . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General approaches to using convex optimization\n",
    "\n",
    "* Pretend/assume/hope $f_i$ are convex (around minimum) and proceed $\\Rightarrow \\textsf{Machine Learning in Practice}$ \n",
    "    - easy on user (problem specifier)\n",
    "    - but lose many benefits of convex optimization\n",
    "    - risky to use in saftey-critical application domain\n",
    "    - an important example `Adam`: one of the most heavily used solver for deep learning.  (also see [Link](https://arxiv.org/pdf/1804.10587.pdf))\n",
    "    \n",
    "    \n",
    "* Verify problem is convex (around minimum) before attempting solution $\\Rightarrow \\textsf{Machine Learning Theory}$ \n",
    "     - but verification for general problem description is hard  \n",
    "     \n",
    "     \n",
    "* Model the problem in a way that results in a convex formulation  $\\Rightarrow \\textsf{Operations Research}$ \n",
    "    - user needs to follow a restricted set of rules and methods\n",
    "    - convexity verification is automatic\n",
    "\n",
    "Each has its advantages, but we focus on 3rd approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can you tell if a problem is convex?\n",
    "\n",
    "* Need to check convexity of a function\n",
    "\n",
    "Approaches:\n",
    "* use basic definition, first or second order conditions, e.g., $\\nabla^2 f(x) \\succeq 0$\n",
    "* via convex calculus: construct $f$ using\n",
    "  - library of basic examples or atoms that are convex\n",
    "  - calculus rules or transformations that preserve convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic convex functions (convex atoms)\n",
    "\n",
    "* $x^{p}$ for $p\\geq1$ or $p\\leq0$; $-x^{p}$ for $0\\leq p\\leq1$\n",
    "\n",
    "* $e^{x},$$-\\log x$, $x\\log x$\n",
    "\n",
    "* $a^{\\top}x+b$\n",
    "\n",
    "* $x^{\\top}x$; $x^{\\top}x/y$ (for $y>0$); $\\sqrt{x^{\\top}x}$\n",
    "\n",
    "* $\\|x\\|$ (any norm)\n",
    "\n",
    "* $\\max(x_{1},\\ldots,x_{n})$\n",
    "\n",
    "* $\\log(e^{x_{1}}+\\ldots+e^{x_{n}})$\n",
    "\n",
    "* $\\log\\det X^{-1}$ (for $X\\succ0$)\n",
    "\n",
    "These are also called *atom*s because they are building block of much more complex convex functions. There are many such atoms, most convex programs in practice can be built from these atoms. A more complete list can be found [here](https://jump.dev/Convex.jl/stable/operations/). \n",
    "\n",
    "When modeling a problem, we aim to construct a model in such a way so that the building blocks of the functions are the atoms above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex calculus rules\n",
    "\n",
    "* nonnegative scaling: if $f$ is convex then $\\alpha f$ is convex\n",
    "if $\\alpha\\geq0$\n",
    "\n",
    "\n",
    "\n",
    "* sum: if $f$ and $g$ are convex, then so is $f+g$\n",
    "\n",
    "\n",
    "\n",
    "* affine composition: if $f$ is convex, then so is $f(Ax+b)$\n",
    "\n",
    "\n",
    "\n",
    "* pointwise maximum: if $f_{1},f_{2},\\ldots,f_{m}$ are convex, then\n",
    "so is $f(x)=\\max_{i\\in\\{1,\\ldots,m\\}}f_{i}(x)$ \n",
    "\n",
    "* pointwise supremum: if $f(x,y)$ is convex in $x$ for all $y \\in S$, then $g(x) = \\textrm{sup}_{y \\in S}{f(x,y)}$ is convex\n",
    "\n",
    "\n",
    "\n",
    "* partial minimization: if $f(x,y)$ is convex in $(x,y)$ and $C$\n",
    "is convex, then $g(x)=\\min_{y\\in C}f(x,y)$ is convex\n",
    "\n",
    "\n",
    "\n",
    "* composition: if $h$ is convex and increasing and $f$ is convex,\n",
    "then $g(x)=h(f(x))$ is convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## The function that may show up in your model can be much more complicated\n",
    "\n",
    "Many complicated convex functions that we can use in the modeling phase can build from the atoms through convex calculus.\n",
    "\n",
    "* piecewise-linear function: $f(x)=\\max_{i=1,\\ldots,k}(a_{i}^{\\top}x+b_{i})$\n",
    "\n",
    "\n",
    "\n",
    "* $\\ell_{1}$-regularized least-squares cost: $\\|Ax-b\\|_{2}^{2}+\\lambda\\|x\\|_{1}$\n",
    "with $\\lambda\\geq0$\n",
    "\n",
    "\n",
    "* support-function of a set: $S_C(x)= \\max_{y \\in C}{x^\\top y}$ where $C$ is any set\n",
    "\n",
    "\n",
    "\n",
    "* distance to convex set: $f(x)=\\min_{y\\in C}\\|x-y\\|_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic modeling steps\n",
    "\n",
    "* When modeling a problem we face many choices how to mathematically model a certain reality\n",
    "\n",
    "\n",
    "* During the modeling phase try to select your functions with their building blocks as convex atoms\n",
    "\n",
    "\n",
    "* When modeling some operation on a mathematical object try to use convex calculus rules\n",
    "\n",
    "\n",
    "* If you can follow the last two steps, you will have a convex problem\n",
    "\n",
    "\n",
    "* If you have a nonconvex problem (that is not mixed integer linear program), you can try one of the two things:\n",
    "   - Work with tightest convex approximation of the nonconvex problem (will see one example)\n",
    "   - *Sequential convex programming*, a local optimization method for nonconvex problems that leverages convex optimization (works suprisingly well in my experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear programs\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\underset{x\\in\\mathbf{R}^{d}}{\\mbox{minimize}} & c^{\\top}x\\\\\n",
    "\\mbox{subject to} & Ax=b,\\\\\n",
    " & Cx\\geq d.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Shows up when we are modeling system that can be completely described by linear equalities and inequalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic convex programs (QCP)\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\underset{x\\in\\mathbf{E}}{\\mbox{minimize}} & c^{\\top}x+x^{\\top}\\underbrace{Q_{0}}_{\\succeq0}x\\\\\n",
    "\\mbox{subject to} & Ax=b,\\\\\n",
    " & x^{\\top}\\underbrace{Q_{i}}_{\\succeq0}x+q_{i}^{\\top}x+r_{i}\\leq0,\\quad i=1,\\ldots,p.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### Where does it show up?\n",
    "\n",
    "QCP mostly show up in data fitting problems such as least-squares and its variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order cone programs (SOCP)\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\underset{x\\in\\mathbf{R}^{d}}{\\mbox{minimize}} & c^{\\top}x\\\\\n",
    "\\mbox{subject to} & \\|A_{i}x+b_{i}\\|_{2}\\leq q_{i}^{\\top}x+d_{i},\\quad i=1,\\ldots,p,\\\\\n",
    " & Cx\\leq f.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "* SOCP is a generalization of LP and QCP\n",
    "*  Allows for linear combinations of variables to be constrained inside a special convex set, called a second-order cone\n",
    "\n",
    "<img src=\"figures/convex/socp.png\" alt=\"image-20220117142714899\" style=\"zoom: 50%;\" />\n",
    "\n",
    "* Shows up in: geometry problems, approximation problems, chance-constrained linear optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semidefinite programs (SDP)\n",
    "\n",
    "Recall that LP in standard form is:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\underset{x\\in\\mathbf{R}^{d}}{\\mbox{minimize}} & c^{\\top}x\\\\\n",
    "\\mbox{subject to} & a_{i}^{\\top}x=b_{i},\\quad i=1,\\ldots,m,\\\\\n",
    " & x\\succeq0.\n",
    "\\end{array}\n",
    "$$\n",
    "An SDP is a generalization of LPs. Standard form of an SDP is:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\underset{X\\in\\mathbf{R}^{d\\times d}}{\\mbox{minimize}} & \\mathbf{tr}(CX)\\\\\n",
    "\\mbox{subject to} & \\mathbf{tr}(A_{i}X)=b_{i},\\quad i=1,\\ldots,m,\\\\\n",
    " & X\\succeq0.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Here $X, C, A_{i}$ are symmetric matrices.\n",
    "\n",
    "SDP shows up in:\n",
    "* sophisticated relaxations (approximations) of non-convex problems,\n",
    "* in control design for linear dynamical systems,\n",
    "*  in system identification,\n",
    "* in algebraic geometry, and\n",
    "* in matrix completion problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mixed-integer linear programs (MILP) (Session 2)\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\underset{x,y}{\\mbox{minimize}} & c^{\\top}x+d^{\\top}y\\\\\n",
    "\\mbox{subject to} & Ax+By=b,\\\\\n",
    " & Cx+Dy\\geq f,\\\\\n",
    " & x\\succeq0,\\\\\n",
    " & x\\in\\mathbf{R}^{d},\\\\\n",
    " & y\\in \\mathbf{Z}^{n}.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "* Nonconvex, but \"practically tractable\" in many situations\n",
    "* MILP solvers have gained a speed-up factor of 2 trillion in the last 40 years!\n",
    "* Wide modeling capability\n",
    "* Usually shows up in situations where\n",
    "  * modeling do/don't do decisions\n",
    "  * logical conditions\n",
    "  * implications (if something happens do this)\n",
    "  * either/or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and solving common convex models in `Julia`\n",
    "\n",
    "### Example of QCP: portfolio optimization\n",
    "\n",
    "\n",
    "Assume that we have a portfolio with $n$ assets at the beginning of time period $t$.\n",
    "\n",
    "Given some forecasts on risks and expected returns we try to find the optimal portfolio that rebalances the portfolio to achieve a good balance between expected risk (variance) $x^\\top \\Sigma x$ and returns $\\mu^\\top x$​​.\n",
    "\n",
    "In it's simplest form we want to solve:\n",
    "$$\n",
    "\\begin{array}{ll} \\text{minimize} &  \t \\gamma (x^\\top \\Sigma x) -\\mu^\\top x \\\\\n",
    "\\text{subject to} &  1^\\top x = d + 1^\\top x^0 \\\\\n",
    "                    &  x  \\geq 0,\n",
    " \\end{array}\n",
    "$$\n",
    "with variable $x \\in \\mathbf{R}^n$, $\\mu$ forecasted (expected) returns, $\\gamma > 0 $ risk aversion parameter.\n",
    "\n",
    "* $x^0_i$ represents the initial investment in asset $i$ and $d$ represents the cash reserve.\n",
    "* The equality constraint tells us that the sum of the new allocation vector $x$​ has to equal the initial allocation plus the cash reserve\n",
    "* the covariance matrix of our risk model is given by $\\Sigma \\in \\mathbf{S}_+^n$​."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages\n",
    "using MosekTools, Mosek, Random, Convex, JuMP, Images, DelimitedFiles \n",
    "# If you do not have Mosek then instead run\n",
    "# using COSMO, Random, Convex, JuMP, Images, DelimitedFiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "include(\"data//convex//portfolio_data.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the problem using `Convex.jl`\n",
    "\n",
    "To solve this problem, we will use `Convex.jl`, which is a specialized `Julia` package for solving convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the variable $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now the define the objective $ \\gamma (x^\\top \\Sigma x) -\\mu^\\top x $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = γ*quadform(x,Σ) - dot(x,μ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the list of all the functions `Convex.jl` can model directly is available at [https://jump.dev/Convex.jl/stable/operations/#Linear-Program-Representable-Functions](https://jump.dev/Convex.jl/stable/operations/#Linear-Program-Representable-Functions).\n",
    "\n",
    "Add the constraint $1^\\top x = d + 1^\\top x^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_linear = ( sum(x) == d + sum(x0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the positivity constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_pos = ( x >= 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, combine everything in a `problem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = minimize(objective, [constraint_linear, constraint_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to solve the problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve!(problem, Mosek.Optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the solution now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show x_sol = Convex.evaluate(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show p_star = Convex.evaluate(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, combining everything together, the code to solve the portfolio optimization problem using `Convex.jl` would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the problem\n",
    "x = Variable(n);\n",
    "objective = γ*quadform(x,Σ) - dot(x,μ);\n",
    "constraint_linear = ( sum(x) == d + sum(x0) );\n",
    "constraint_pos = ( x >= 0 );\n",
    "problem = minimize(objective, [constraint_linear, constraint_pos]);\n",
    "\n",
    "## Solve the problem\n",
    "solve!(problem, Mosek.Optimizer);\n",
    "\n",
    "## Extract the solution\n",
    "x_sol = Convex.evaluate(x);\n",
    "p_star = Convex.evaluate(objective);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the problem using `JuMP`\n",
    "\n",
    "There is nothing special solving a problem via `Convex.jl` over `JuMP`. We could solve the same problem using `JuMP` with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the problem\n",
    "using JuMP\n",
    "model = Model(Mosek.Optimizer)\n",
    "@variable(model, xs[1:n] >= 0)\n",
    "@objective(model, Min,  γ * xs'*Σ*xs  - μ' * xs);\n",
    "@constraint(model, sum(xs) .== d + sum(x0))\n",
    "\n",
    "## Solve the problem\n",
    "optimize!(model)\n",
    "\n",
    "## Get the optimal solution\n",
    "x_sol_JuMP = value.(xs)\n",
    "p_star_JuMP = objective_value(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we used the same solver, we hope to get the same optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(x_sol - x_sol_JuMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use `JuMP` vs `Convex.jl`?\n",
    "\n",
    "* Usually `JuMP`'s model creation time is somewhat faster than `Convex.jl`, so if you are working on a problem, where performance is key, then using `JuMP` is a good idea.\n",
    "* If you are in the prototyping phase of your model and you feel that you are working with a lot of convex-ish functions, then working with `Convex.jl` might be a good idea.\n",
    "  * For example, suppose, in the last problem you are interested in finding a sparse portfolio with the risk being less than some bound. Within convex optimization framework, one way doing this is by minimizing the $\\ell_1$ norm of $x$, i.e., consider the modified objective  $-\\mu^\\top x + 0.1 \\cdot \\| x\\|_1$ (to balance the $\\mu^\\top x$ and the sparsity) and the additional constraint $x^\\top \\Sigma x \\leq \\delta := 0.5$ (to ensure the expected risk no larger than $\\delta$). If we want to do this modification via `JuMP`, then the right approach is reformulate the  $\\ell_1$​ norm via epigraph etc, but `Convex.jl` will take this term as it is.\n",
    "\n",
    "\n",
    "$\\|x\\|_1$ = `norm(x,1)`\n",
    "\n",
    "$x^\\top \\Sigma x$ = `quadform(x_sparse,Σ)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill in the 😕s\n",
    "δ = 0.5\n",
    "x_sparse = Variable(n)\n",
    "objective_sparse = 😕\n",
    "constraint_linear = ( sum(x_sparse) == d + sum(x0) )\n",
    "constraint_pos = ( x_sparse >= 0 )\n",
    "constraint_additional = ( 😕 <= δ)\n",
    "problem_sparse = minimize(objective_sparse, [constraint_linear, constraint_pos, constraint_additional])\n",
    "\n",
    "## Solve the problem\n",
    "solve!(problem_sparse, Mosek.Optimizer)\n",
    "\n",
    "## Extract the solution\n",
    "x_sol_sparse = Convex.evaluate(x_sparse)\n",
    "p_star_sparse = Convex.evaluate(objective_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "δ = 0.5\n",
    "x_sparse = Variable(n)\n",
    "objective_sparse =   - dot(x_sparse, μ) + 0.1 * norm(x_sparse,1)\n",
    "traint_linear = ( sum(x_sparse) == d + sum(x0) )\n",
    "constraint_pos = ( x_sparse >= 0 )\n",
    "constraint_additional = ( quadform(x_sparse,Σ) <= δ)\n",
    "problem_sparse = minimize(objective_sparse, [constraint_linear, constraint_pos, constraint_additional])\n",
    "\n",
    "## Solve the problem\n",
    "solve!(problem_sparse, Mosek.Optimizer)\n",
    "\n",
    "## Extract the solution\n",
    "x_sol_sparse = Convex.evaluate(x_sparse)\n",
    "p_star_sparse = Convex.evaluate(objective_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "plot([1:n],[x_sol_JuMP x_sol_sparse], xlabel = \"k\", ylabel = \"x[k]\", label = [ \"x original\" \"x sparse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of SOCP: time series analysis\n",
    "\n",
    "A time series is a sequence of data points, each associated with a time. In our example, we will work with a time series of daily temperatures in the city of Melbourne, Australia over a period of a few years. Let $\\tau$ be the vector of the time series, and $\\tau_i$ denote the temperature in Melbourne on day $i$. Here is a picture of the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles, Plots\n",
    "τ = readdlm(\"data//convex//melbourne_temps.txt\", ',')\n",
    "n = size(τ, 1)\n",
    "plot(1:n, τ[1:n], ylabel=\"Temperature (°C)\", label=\"data\", xlabel = \"Time (days)\", xticks=0:365:n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to model this time series would be to find a smooth curve that approximates the yearly ups and downs.\n",
    "We can represent this model as a vector $x$ where $x_i$ denotes the predicted temperature on the $i$-th day.\n",
    "To force this trend to repeat yearly, we simply want to impose the constraint\n",
    "\n",
    "$$\n",
    " x_i = x_{i + 365}\n",
    "$$\n",
    "\n",
    "for each applicable $i$.\n",
    "\n",
    "We also want our model to have two more properties:\n",
    "\n",
    "- The first is that the temperature on each day in our model should be relatively close to the actual temperature of that day and equal if possible.\n",
    "- The second is that our model needs to be smooth, so the change in temperature from day to day should be relatively small. The following objective would capture both properties:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n}|x_{i}-\\tau_{i}|+\\lambda\\sqrt{\\sum_{i=2}^{n}(x_{i}-x_{i-1})^{2}}=\\|x-\\tau\\|_{1}+\\lambda\\|Ax\\|_{2},\n",
    "$$\n",
    " where $A$ is a matrix of size $(n-1)\\times n$ with $A_{i,i}=-1,A_{i,i+1}=1$\n",
    "for $i=1,\\ldots,n-1$ and the rest of the elements being zero. So,\n",
    "the optimization problem we want to solve is: \n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\underset{x\\in\\mathbf{R}^{d}}{\\mbox{minimize}} & \\|x-\\tau\\|_{1}+\\lambda\\|Ax\\|_{2}\\\\\n",
    "\\mbox{subject to} & x_{i}=x_{i+365},\\quad i=1,\\ldots,n.\n",
    "\\end{array}\n",
    "$$\n",
    "This is an SOCP, because it can be written as: \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\underset{x,u,t}{\\mbox{minimize}} & \\sum_{i=1}^{n}u_{i}+\\lambda t\\\\\n",
    "\\mbox{subject to} & x_{i}=x_{i+365},\\quad i=1,\\ldots,n, \\\\\n",
    "& \\|Ax\\|_{2}\\leq t,\\\\\n",
    " & \\vert x_{i}-\\tau_{i}\\vert\\leq u_{i},\\quad i=1,\\ldots,n.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Here, $\\lambda$ is the smoothing parameter. The larger $\\lambda$ is, the smoother our model will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will solve this problem using `Convex.jl`, because it would allow us to input the problem directly without manually converting into the SOCP format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution using Convex.jl\n",
    "x = Variable(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_constraints = [ x[i] == x[i - 365] for i in 365 + 1 : n ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 1 # smoothing parameter\n",
    "# Define the matrix A\n",
    "A = zeros(n-1,n)\n",
    "for i in 1:n-1\n",
    "    A[i,i] = -1\n",
    "    A[i,i+1] = 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_objective = norm(x-τ,1) + λ*norm(A*x,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_problem = minimize(smooth_objective, eq_constraints);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve!(smooth_problem, Mosek.Optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our smoothed time estimate vs the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot smooth fit\n",
    "plot(1:n, τ[1:n], label=\"data\")\n",
    "plot!(1:n, Convex.evaluate(x)[1:n], linewidth=2, label=\"smooth fit\",  ylabel=\"Temperature (°C)\", xticks=0:365:n, xlabel=\"Time (days)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix completion problem: how to reconstruct a distorted image\n",
    "\n",
    "Suppose we are given a noisy image, and we want to clean the image up. In countless movies and tv shows, we have seen our hero figuring out who the killer is by polishing a very noisy image in seconds. In reality, this takes a while, and usually is done by solving an SDP. \n",
    "\n",
    "Let's take a look at an example. Suppose we are given this very noisy image. This image is the noisy version of a test image widely used in the field of image processing since 1973. See the wikipedia entry [here](https://en.wikipedia.org/wiki/Lenna) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Images\n",
    "lenna = load(\"figures//convex//lena128missing.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noisy image is basically a matrix, where lot of the pixels are not reliable. In a way, these unreliable pixels can be treated as missing entries of a matrix, because any $n\\times n$ image is nothing but a $n \\times n$ matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we convert the $128\\times128$ image into a matrix. Here, through some precalculation, I have already filled in the missing entries with zero (for the sake of illustration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to real matrices\n",
    "Y = Float64.(lenna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal \n",
    "Our goal is to find a the missing entries of this matrix $Y$ and thus reconstruct the image and hopefully figure out who this person is. \n",
    "\n",
    "#### Constraint to impose\n",
    "* Ofcourse, we want to ensure that in the reconstructed image, call it $X$, for the available pixels, both images agree, i.e., for any $(i,j)$ in the index set of observed entries, we have $X_{i,j} = Y_{i,j}$. \n",
    "\n",
    "#### But how to fill in the rest of the entries? \n",
    "\n",
    "One reasonable of way of doing it finding the simplest image that fits the observed entries. Simplicity of an image when traslated to a matrix can correspond to a matrix with low rank. In other words, we want to minimize the rank of the decision matrix $X$, but subject to $X_{i,j} = Y_{i,j}$ for the observed pixels.\n",
    "\n",
    "\\begin{array}{ll}\n",
    "\\underset{X}{\\mbox{minimize}} & \\text{rank}(X)\\\\\n",
    "\\mbox{subject to} & X_{i,j}=Y_{i,j},\\quad(i,j)\\in\\text{observed pixels of }Y\n",
    "\\end{array}\n",
    "\n",
    "But this is a very hard problem and solving to certifiable global optimality is an active research area. As of now, problems beyond matrix size of $50\\times 50$ cannot be solved in a tractable fashion. You can see Ryan Cory-Wright's paper on how to solve low-rank problems to certifiable global optimality [here](http://www.optimization-online.org/DB_FILE/2020/09/8031.pdf).\n",
    "\n",
    "#### The best convex approximation of the problem\n",
    "\n",
    "To work around this issue, rather than minimizing $\\textrm{rank}(X)$ we minimize the best convex approximation of the rank function, which is the nuclear norm of $X$, denoted by $\\| X \\|_\\star$, which is equal to the sum of singular values of the matrix $X$. So, we solve:\n",
    "\n",
    "\\begin{array}{ll}\n",
    "\\underset{X}{\\mbox{minimize}} & \\| X \\|_\\star \\\\\n",
    "\\mbox{subject to} & X_{i,j}=Y_{i,j},\\quad(i,j)\\in\\text{observed pixels of }Y\n",
    "\\end{array}\n",
    "\n",
    "The problem above can be formulated as an SDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the index set of the observed entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_entries_Y = findall(x->x!=0.0, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find size of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, N = size(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the variable $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(N,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the objective $\\| X \\|_\\star$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_SDP = nuclearnorm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the constraints $X_{i,j} = Y_{i,j}$ for all $(i,j) \\in \\texttt{observed_entries_Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_SDP = Convex.Constraint[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_i_j in observed_entries_Y\n",
    "    i = index_i_j[1]\n",
    "    j = index_i_j[2]\n",
    "    push!(constraints_SDP,  X[i,j] == Y[i,j])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the problem now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_SDP = minimize(obj_SDP, constraints_SDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve!(problem_SDP, Mosek.Optimizer)\n",
    "\n",
    "# [💀] If you want to run it on your laptop, it needs about 30GB RAM and 937 seconds. \n",
    "# So think AGAIN!\n",
    "#\n",
    "# Constraints            : 40769           \n",
    "# Scalar variables       : 49153 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Problem\n",
    "  Name                   :                 \n",
    "  Objective sense        : minimize        \n",
    "  Type                   : CONIC (conic optimization problem)\n",
    "  Constraints            : 40769           \n",
    "  Affine conic cons.     : 1 (32896 rows)\n",
    "  Disjunctive cons.      : 0               \n",
    "  Cones                  : 0               \n",
    "  Scalar variables       : 49153           \n",
    "  Matrix variables       : 0               \n",
    "  Integer variables      : 0               \n",
    "\n",
    "Optimizer started.\n",
    "Presolve started.\n",
    "Linear dependency checker started.\n",
    "Linear dependency checker terminated.\n",
    "Eliminator started.\n",
    "Freed constraints in eliminator : 0\n",
    "Eliminator terminated.\n",
    "Eliminator started.\n",
    "Freed constraints in eliminator : 0\n",
    "Eliminator terminated.\n",
    "Eliminator - tries                  : 2                 time                   : 0.00            \n",
    "Lin. dep.  - tries                  : 1                 time                   : 0.01            \n",
    "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
    "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
    "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
    "Presolve terminated. Time: 0.04    \n",
    "GP based matrix reordering started.\n",
    "GP based matrix reordering terminated.\n",
    "Optimizer  - threads                : 10              \n",
    "Optimizer  - solved problem         : the primal      \n",
    "Optimizer  - Constraints            : 32896           \n",
    "Optimizer  - Cones                  : 1               \n",
    "Optimizer  - Scalar variables       : 24769             conic                  : 24769           \n",
    "Optimizer  - Semi-definite variables: 1                 scalarized             : 32896           \n",
    "Factor     - setup time             : 72.44           \n",
    "Factor     - dense det. time        : 56.18             GP order time          : 0.01            \n",
    "Factor     - nonzeros before factor : 5.41e+08          after factor           : 5.41e+08        \n",
    "Factor     - dense dim.             : 0                 flops                  : 1.19e+13        \n",
    "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
    "0   1.3e+00  1.0e+00  1.0e+00  0.00e+00   0.000000000e+00   0.000000000e+00   1.0e+00  72.52 \n",
    "1   4.8e-01  3.6e-01  5.9e-01  -9.71e-01  1.415377135e+02   1.431598397e+02   3.6e-01  150.96\n",
    "2   4.0e-01  3.1e-01  3.4e-01  -3.34e-01  1.116403593e+02   1.121811844e+02   3.1e-01  239.37\n",
    "3   7.0e-02  5.3e-02  1.1e-02  2.56e-01   1.532518635e+02   1.531753559e+02   5.3e-02  325.83\n",
    "4   5.9e-03  4.5e-03  5.5e-04  1.07e+00   1.479916370e+02   1.479971145e+02   4.5e-03  410.94\n",
    "5   1.3e-03  1.0e-03  5.8e-05  1.00e+00   1.481214349e+02   1.481225967e+02   1.0e-03  496.76\n",
    "6   2.8e-05  2.2e-05  5.0e-08  1.00e+00   1.479726003e+02   1.479725623e+02   2.2e-05  583.75\n",
    "7   4.6e-06  3.5e-06  1.2e-08  1.00e+00   1.479718423e+02   1.479718468e+02   3.5e-06  671.28\n",
    "8   4.9e-07  3.7e-07  4.2e-10  1.00e+00   1.479711248e+02   1.479711253e+02   3.7e-07  758.13\n",
    "9   7.4e-08  4.1e-08  1.3e-11  1.00e+00   1.479710892e+02   1.479710892e+02   3.5e-08  846.89\n",
    "10  6.6e-10  7.2e-10  8.2e-15  1.00e+00   1.479710824e+02   1.479710824e+02   3.1e-10  936.97\n",
    "Optimizer terminated. Time: 937.14 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal solution will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Serialization\n",
    "X_sdp_sol = deserialize(\"data//convex//optimal_X.dat\");\n",
    "\n",
    "# If you are not cheating like me, you should run:\n",
    "# X_sdp_sol = Convex.evaluate(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lenna colorview(Gray, X_sdp_sol)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare with the test set (original image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenna_original = load(\"figures//convex//Lenna_(test_image).png\")\n",
    "[lenna colorview(Gray, X_sdp_sol) lenna_original]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had formulated and solve the problem using `JuMP` (this will be the homework), the time taken by it would be around 20 s, whereas time taken by `Convex.jl` is 937 s. The main reason behind the time difference is that, to use `Convex.jl` we could feed it the problem formulation rather than converting into an SDP form, so internally `Convex.jl` converts the model into an SDP programmatically. The converted SDP has the follwoing size:\n",
    "\n",
    "```\n",
    "The SDP size in Convex.jl (constructed internally Convex.jl) \n",
    "#-----------------------------------------------------------\n",
    " Constraints            : 40769                      \n",
    " Scalar variables       : 49153\n",
    "```\n",
    "\n",
    "Whereas, for `JuMP` we converted the problem ourselves into an SDP to feed it into `JuMP`. The final formulation would be much tighter than what `Convex.jl` does automatically. Note that we invested some time here by researching into google scholar and finding the right paper. The SDP in `JuMP` has the following size:\n",
    " \n",
    "```\n",
    "The SDP size in JuMP \n",
    "#------------------------------\n",
    "  Constraints            : 8129                       \n",
    "  Scalar variables       : 16257  \n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential convex programming\n",
    "\n",
    "* Solving a nonconvex problem using a local convex optimization method\n",
    "\n",
    "\n",
    "\n",
    "* Convex portions of a problem are handled \"exactly\" and efficiently\n",
    "\n",
    "\n",
    "\n",
    "* Sequential convex programming is a heuristic, it can fail\n",
    "\n",
    "\n",
    "\n",
    "* Success often depend on a good starting point\n",
    "\n",
    "\n",
    "\n",
    "* We consider the nonconvex problem: \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\underset{x\\in\\mathbf{R}^{d}}{\\mbox{minimize}} & f_{0}(x)\\\\\n",
    "\\mbox{subject to} & f_{i}(x)\\leq0,\\quad i=1,\\ldots,m,\\\\\n",
    " & h_{i}(x)=0,\\quad j=1,\\ldots,p.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "where $f_{0}$ and $f_{i}$ are possibly nonconvex, $h_{i}$ are\n",
    "possibly non-affine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic idea of Sequential convex programming\n",
    "\n",
    "* Maintain the current iterate $x^{(k)}$ and convex trust region $\\mathcal{T}^{(k)}$\n",
    "\n",
    "* Form convex approximation $f_{i}^{\\textrm{cvx}}$ of $f_{i}$ over\n",
    "$\\mathcal{T}^{(k)}$\n",
    "\n",
    "* Form affine approximation $h_{i}^{\\textrm{afn}}$ of $h_{i}$ over\n",
    "$\\mathcal{T}^{(k)}$\n",
    "\n",
    "* Then update the iterate $x^{(k+1)}$ is the optimal point found by\n",
    "solving the convex problem \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\underset{x\\in\\mathbf{R}^{d}}{\\mbox{minimize}} & f_{0}^{\\textrm{cvx}}(x)\\\\\n",
    "\\mbox{subject to} & f_{i}^{\\textrm{cvx}}(x)\\leq0,\\quad i=1,\\ldots,m,\\\\\n",
    " & h_{i}^{\\textrm{afn}}(x)=0,\\quad j=1,\\ldots,p,\\\\\n",
    " & x\\in\\mathcal{T}^{(k)},\n",
    "\\end{array}\n",
    "$$\n",
    "which is a convex approximation of the original nonconvex problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute the approximations\n",
    "\n",
    "Trust region is computed using $\\mathcal{T}^{(k)}=\\{x\\mid\\|x-x^{(k)}\\|\\leq\\rho\\}$\n",
    "\n",
    "* \n",
    "$$\n",
    "h_{i}^{\\textrm{afn}}=h_{i}(x^{(k)})+\\nabla h_{i}(x^{(k)})^{\\top}(x-x^{(k)})\n",
    "$$\n",
    "\n",
    "* \n",
    "$$f_{i}^{\\textrm{cvx}}=f_{i}(x^{(k)})+\\nabla f_{i}(x^{(k)})^{\\top}(x-x^{(k)})+\\frac{1}{2}(x-x^{(k)})^{\\top}P(x-x^{(k)})$$\n",
    "\n",
    "where $P=\\left[\\nabla^{2}f(x^{(k)})\\right]_{+}$ which is the PSD\n",
    "part of Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
